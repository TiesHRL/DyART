{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import det, inv\n",
    "from scipy.special import gamma\n",
    "from math import pi, ceil\n",
    "from scipy.special import erfinv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# from statsmodels.tsa import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from random import randint\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import pymc as pm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_df_multivar(n_samples=200, n_exog_vars=4):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset with a clear relationship between the target and multiple exogenous variables.\n",
    "    Returns the data as pandas DataFrames.\n",
    "    \"\"\"\n",
    "    exog_data = {}\n",
    "    \n",
    "    for i in range(n_exog_vars):\n",
    "        # Exogenous variable\n",
    "        exog_data[i] = np.linspace(0, 10*(i+1), n_samples) + np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Create a DataFrame for exogenous variables\n",
    "    exog_df = pd.DataFrame(exog_data)\n",
    "    \n",
    "    # Target variable influenced by the sum of exogenous variables with some noise\n",
    "    target = exog_df.sum(axis=1) + 5 * np.sin(exog_df[0]) + np.random.normal(0, 2, n_samples)\n",
    "    target_df = pd.DataFrame(target, columns=[0])\n",
    "    \n",
    "    return target_df, exog_df\n",
    "\n",
    "def plot_data_distributions_grid(target_df, exog_df):\n",
    "    \"\"\"\n",
    "    Plots the distributions of the target and multiple exogenous variables in a grid layout.\n",
    "    \"\"\"\n",
    "    n_exog_vars = exog_df.shape[1]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_exog_vars+1, figsize=(8, 3))\n",
    "\n",
    "    # Plotting target variable distribution\n",
    "    target_df[0].hist(ax=axes[0], bins=30, color='blue', alpha=0.3)\n",
    "    axes[0].set_title(\"Target Variable Distribution\")\n",
    "    axes[0].set_xlabel(\"Value\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Plotting exogenous variable distributions\n",
    "    for i, col in enumerate(exog_df.columns):\n",
    "        exog_df[col].hist(ax=axes[i+1], bins=30, color='green', alpha=0.3)\n",
    "        axes[i+1].set_title(f\"{col} Distribution\")\n",
    "        axes[i+1].set_xlabel(\"Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate synthetic data with multiple exogenous variables as DataFrames\n",
    "# synthetic_target_df_multivar, synthetic_exog_df_multivar = generate_synthetic_data_df_multivar()\n",
    "\n",
    "# Plot distributions for the synthetic data\n",
    "synthetic_target, synthetic_exog = generate_synthetic_data_df_multivar()\n",
    "# print(len(ts_train),len(ts_train[0]))#,len(ts_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, method):\n",
    "    ts_train = []\n",
    "    ts_valid = []\n",
    "    ts_test = []\n",
    "    ts_param = []\n",
    "    flag_n = False\n",
    "    flag_diff = False\n",
    "    flag_dec = False\n",
    "    if method is 'normalization':\n",
    "        flag_n = True\n",
    "    elif method is 'differencing':\n",
    "        flag_diff = True\n",
    "            \n",
    "    for i in range(data.shape[1]):\n",
    "\n",
    "        temp = list(data.iloc[6:][i].dropna())\n",
    "        # print(temp)\n",
    "        if len(temp) > 130:\n",
    "            \n",
    "            cut_off_1 = ceil(len(temp)*0.7)\n",
    "            cut_off_2 = ceil(len(temp)*0.9)\n",
    "\n",
    "            temp_train = temp[:cut_off_1]\n",
    "            temp_val = temp[cut_off_1:cut_off_2]\n",
    "            temp_test = temp[cut_off_2:]\n",
    "\n",
    "            if flag_n is True:\n",
    "                ts_param.append([np.mean(temp_train), np.std(temp_train), np.mean(temp_val), \n",
    "                np.std(temp_val), np.mean(temp_test), np.std(temp_test)])\n",
    "                \n",
    "                temp_train = (temp_train - np.mean(temp_train)) / np.std(temp_train)\n",
    "                ts_train.append(temp_train)\n",
    "                \n",
    "\n",
    "                temp_val = (temp_val - np.mean(temp_val)) / np.std(temp_val)\n",
    "                ts_valid.append(temp_val)\n",
    "\n",
    "                temp_test = (temp_test - np.mean(temp_test)) / np.std(temp_test)\n",
    "                ts_test.append(temp_test)\n",
    "                \n",
    "\n",
    "            elif flag_diff is True:\n",
    "\n",
    "                temp_train = pd.Series(temp_train)\n",
    "                temp_train_log_diff = temp_train - temp_train.shift()\n",
    "                temp_train_log_diff[0] = temp_train[0]\n",
    "                temp_train_log_diff.dropna(inplace=True)\n",
    "                ts_train.append(temp_train_log_diff.values)\n",
    "\n",
    "                temp_val = pd.Series(temp_val)\n",
    "                temp_val_log_diff = temp_val - temp_val.shift()\n",
    "                temp_val_log_diff[0] = temp_val[0]\n",
    "                temp_val_log_diff.dropna(inplace=True)\n",
    "                ts_valid.append(temp_val_log_diff.values)\n",
    "\n",
    "                temp_test = pd.Series(temp_test)\n",
    "                temp_test_log_diff = temp_test - temp_test.shift()\n",
    "                temp_test_log_diff[0] = temp_test[0]\n",
    "                temp_test_log_diff.dropna(inplace=True)\n",
    "                ts_test.append(temp_test_log_diff.values)\n",
    "                \n",
    "    return ts_train, ts_valid, ts_test, ts_param\n",
    "\n",
    "# ts_train, ts_valid, ts_test, ts_param = preprocessing(df_temp, method='normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveTree:\n",
    "    \n",
    "    def __init__(self, p, u0=0, alpha_u=1, X=None):\n",
    "        self._X = X  # Exogenous data\n",
    "        \n",
    "        erf_temp = np.zeros([7,1])\n",
    "        for i in range(1,8):\n",
    "            erf_temp[i-1] = erfinv((i/4) - 1)\n",
    "        \n",
    "        self._erf = erf_temp\n",
    "        self._p = p\n",
    "        self._alpha_W = p + 2\n",
    "\n",
    "        self._u0 = u0\n",
    "        self._alpha_u = alpha_u\n",
    "        self.target = []\n",
    "        self.exog = []\n",
    "    #  calculate the sample mean of the data (could be a vector), maybe split into sample mean of each variable\n",
    "    def sample_mean(self, data):\n",
    "        # print(sum(data), len(data))\n",
    "        return sum(np.asarray(data))/len(data)\n",
    "\n",
    "    \n",
    "    # calculate the scatter matrix around the mean uN\n",
    "    # def scatter_matrix(self, data, uN_):\n",
    "    def scatter_matrix(self, data, uN_):\n",
    "\n",
    "        # assert data.shape[1] == p * (1 + no_exog_vars), 'Data dimensions do not match expected shape'\n",
    "        # Assuming data has been preprocessed to include lags of y and X\n",
    "        temp = data - uN_\n",
    "\n",
    "        SN = 0\n",
    "        for row in temp:\n",
    "            row = row[:, np.newaxis]\n",
    "            SN += row * row.T\n",
    "        # print(len(SN),len(SN[0]))\n",
    "        return SN\n",
    "    # def WN_func(self, uN_, SN, W0, N):\n",
    "    def WN_func(self, uN_, SN, W0, N):\n",
    "        # assert SN.shape[0] == p * (1 + no_exog_vars), 'Scatter matrix dimensions do not match expected shape'\n",
    "        temp = self._u0 - uN_\n",
    "\n",
    "        # Assuming scatter matrix has been computed from preprocessed data\n",
    "        temp = temp[:, np.newaxis]\n",
    "        WN = W0 + SN + ((self._alpha_u * N) / (self._alpha_u + N)) * np.dot(temp, temp.T)\n",
    "        return WN\n",
    "    # Updates the matrix WN_d, calculates the within node covariance matrix \n",
    "    def WN_d_func(self, u0_, uN_d_, SN_d_, W0_, N_):\n",
    "        temp = u0_ - uN_d_\n",
    "        temp = temp[:, np.newaxis]\n",
    "        WN_ = W0_ + SN_d_ + ((self._alpha_u * N_) / (self._alpha_u + N_)) * np.dot(temp, temp.T)\n",
    "        return WN_\n",
    "    # calculating the Maximum a posteriori arameters for the ar model\n",
    "    def MAP_param(self, N, uN_, WN):\n",
    "        ut = ((self._alpha_u * self._u0) + (N * uN_)) / (self._alpha_u + N)\n",
    "        Wt_inv = (1 / (self._alpha_W + N - (self._p + 1))) * WN\n",
    "        return ut, Wt_inv\n",
    "    # calculate all of the AR parameters needed \n",
    "    def param(self, target, exog):\n",
    "        # exog = np.array(exog)\n",
    "        # exog2 = np.reshape(exog.transpose(1, 0, 2), (len(exog[0]), len(exog)*len(exog[0][0])))\n",
    "        # data = np.hstack((target, exog2))\n",
    "        self.target, self.exog = target, exog\n",
    "        \n",
    "        data = []\n",
    "        for t, e in zip(target, np.asarray(exog).transpose(1, 0, 2)):\n",
    "                data.append(np.concatenate([t, e.flatten()]))\n",
    "        \n",
    "        data = np.array(data)\n",
    "        y = data[:, 0]\n",
    "        y_lags = data[:, 1:self._p + 1]\n",
    "        x_lags = data[:, self._p + 1:]\n",
    "        \n",
    "        with pm.Model() as arx_model:\n",
    "            # Priors\n",
    "            intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
    "            phi = pm.Normal('phi', mu=0, sigma=10, shape=self._p)\n",
    "            theta = pm.Normal('theta', mu=0, sigma=10, shape=data.shape[1]-self._p-1)\n",
    "            sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "            \n",
    "            # Expected value\n",
    "            mu = intercept + pm.math.dot(y_lags, phi) + pm.math.dot(x_lags, theta)\n",
    "            \n",
    "            # Likelihood\n",
    "            y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n",
    "            \n",
    "            # Compute the MAP estimate\n",
    "            map_estimate = pm.find_MAP(progressbar=False)\n",
    "\n",
    "        # m = map_estimate['intercept']\n",
    "        # b = np.append(map_estimate['phi'], map_estimate['theta'])\n",
    "        # var = map_estimate['sigma']\n",
    "        \n",
    "        # Concatenate list1 and reshaped list2\n",
    "        # print(\"datalen:\", np.asarray(data).shape)\n",
    "        N = len(data)\n",
    "        uN_ = self.sample_mean(data)\n",
    "        SN = self.scatter_matrix(data, uN_)\n",
    "        W0 = np.identity(SN.shape[0])\n",
    "        WN = self.WN_func(uN_, SN, W0, N)\n",
    "        ut, Wt_inv = self.MAP_param(N, uN_, WN)\n",
    "        W = inv(Wt_inv)\n",
    "        var = 1 / W[-1, -1]\n",
    "        Wpp = inv(Wt_inv[:-1, :-1])\n",
    "        # print(Wpp.shape)\n",
    "        b = np.zeros([len(data[0]), 1])\n",
    "        for j in range(len(b)-1):\n",
    "            for i in range(self._p):\n",
    "                b[j] += Wt_inv[-1, i] * Wpp[i, j]\n",
    "        b = b[:-1]\n",
    "        m = ut[-1]\n",
    "        for i in range(len(b)):\n",
    "            m += b[i] * ut[i]\n",
    "        \n",
    "        return var, b, m[0]\n",
    "    # scaling function using the gamma distribution\n",
    "    def c_func(self, l, alpha):\n",
    "        c = 1\n",
    "        #   for loop goes from 1 to l\n",
    "        for i in range(1, l + 1):\n",
    "            c *= gamma((alpha + 1 - i) / 2)\n",
    "        \n",
    "        return c\n",
    "    # probability density scaling function used\n",
    "    def pds_func(self, N, W0, WN):\n",
    "        pds = (pi**(-((self._p + 1) * N) / 2)) + \\\n",
    "        ((self._alpha_u / (self._alpha_u + N))**((self._p + 1) / 2)) + \\\n",
    "        (self.c_func(self._p + 1, self._alpha_W + N) / self.c_func(self._p + 1, self._alpha_W)) * (det(W0)**(self._alpha_W / 2))*(det(WN)**(-(self._alpha_W + N) / 2))\n",
    "        return pds\n",
    "    # similiar to above just now with different params\n",
    "    def pd_s_func(self, u0_, N_, W0_, WN_):\n",
    "        pds = (pi**(-((self._p + 1) * N_) / 2)) + \\\n",
    "        ((self._alpha_u / (self._alpha_u + N_))**((self._p + 1) / 2)) + \\\n",
    "        (self.c_func(self._p + 1, self._alpha_W - 1 + N_) / self.c_func(self._p + 1, self._alpha_W - 1)) * (det(W0_)**((self._alpha_W - 1) / 2))*(det(WN_)**(-(self._alpha_W - 1 + N_) / 2))\n",
    "        return pds\n",
    "    def mult_func(self, l, alpha, N):\n",
    "        c = 1\n",
    "        #   for loop goes from 1 to l\n",
    "        for i in range(1, l + 1):\n",
    "            c *= ((alpha + 1 + N - i)/(alpha + 1 - i))\n",
    "        \n",
    "        return c\n",
    "\n",
    "    def pds_func2(self, N, W0, WN, u0_, N_, W0_, WN_):\n",
    "\n",
    "        pds = (det(W0)**(self._alpha_W / 2))*det(WN)**(-(self._alpha_W + N) / 2) / (det(W0_)**((self._alpha_W - 1) / 2))*(det(WN_)**(-(self._alpha_W - 1 + N_) / 2)) * \\\n",
    "        self.mult_func(self._p + 1,self._alpha_W, N)                                                                                                             \n",
    "\n",
    "        return pds\n",
    "\n",
    "    def LeafScore(self, data):\n",
    "        N = len(data)\n",
    "        self.target = data\n",
    "        uN_ = self.sample_mean(data)\n",
    "        SN = self.scatter_matrix(data, uN_)\n",
    "        W0 = np.identity(SN.shape[0])\n",
    "        WN = self.WN_func(uN_, SN, W0, N)\n",
    "        ut, Wt_inv = self.MAP_param(N, uN_, WN)\n",
    "        data_ = []\n",
    "        for x in data:\n",
    "            data_.append(x[:-1])\n",
    "        N_ = len(data_)\n",
    "        uN_d_ = self.sample_mean(data_)\n",
    "        SN_d_ = self.scatter_matrix(data_, uN_d_)\n",
    "        u0_ = ut[:-1]\n",
    "        W0_ = inv(inv(W0)[:-1, :-1])\n",
    "        WN_d_ = self.WN_d_func(u0_, uN_d_, SN_d_, W0_, N_)\n",
    "\n",
    "        if N > 20:\n",
    "            pds2 = self.pds_func2(N, W0, WN, u0_, N_, W0_, WN_d_)\n",
    "        else:\n",
    "            pds = self.pds_func(N, W0, WN)\n",
    "            pds_ = self.pd_s_func(u0_, N_, W0_, WN_d_)\n",
    "            pds2 =  pds/pds_\n",
    "        # print(\"frac: \", pds/pds_)\n",
    "        # print(\"frac2: \" , pds2)\n",
    "        return pds2\n",
    "        \n",
    "    # This will spplit a dataset into two froups based on the specific features. Then splits the data points into the left or right set\n",
    "    def test_split(self, index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "    def rest_split(self, index, value, train, exog, ex, var):\n",
    "                # Initializations\n",
    "        left, right = [], []\n",
    "        left_e, right_e = [], []\n",
    "\n",
    "        # Convert to DataFrame for easier operations\n",
    "        train_df = pd.DataFrame(train)\n",
    "        exog_dfs = [pd.DataFrame(ex) for ex in exog]\n",
    "\n",
    "        # Split based on exogenous variable\n",
    "        if ex == \"y\":\n",
    "            # Split the specified variable in exog\n",
    "            for i, row in exog_dfs[var].iterrows():\n",
    "                if row[index] < value:\n",
    "                    left_e.append(i)\n",
    "                else:\n",
    "                    right_e.append(i)\n",
    "\n",
    "            left_e_index = pd.Index(left_e)\n",
    "            right_e_index = pd.Index(right_e)\n",
    "\n",
    "            # Extract corresponding rows from other variables in exog and from train\n",
    "            left_exog = [df.loc[left_e_index] for df in exog_dfs]\n",
    "            right_exog = [df.loc[right_e_index] for df in exog_dfs]\n",
    "\n",
    "            left = train_df.loc[left_e_index]\n",
    "            right = train_df.loc[right_e_index]\n",
    "\n",
    "        # Split based on training data\n",
    "        else:\n",
    "            for i, row in train_df.iterrows():\n",
    "                if row[index] < value:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "            \n",
    "            left_index = pd.Index(left)\n",
    "            right_index = pd.Index(right)\n",
    "\n",
    "            left = train_df.loc[left_index]\n",
    "            right = train_df.loc[right_index]\n",
    "\n",
    "            left_exog = [df.loc[left_index] for df in exog_dfs]\n",
    "            right_exog = [df.loc[right_index] for df in exog_dfs]\n",
    "        left = left.values.tolist()\n",
    "        right = right.values.tolist()\n",
    "        left_exog = [df.values.tolist() for df in left_exog]\n",
    "        right_exog = [df.values.tolist() for df in right_exog]\n",
    "        return left, right, left_exog, right_exog\n",
    "    # itrates through the features and the values to det the best for splitting the dataset, calkculates the score for each split and choses the one with best improvement\n",
    "    def get_split(self, train, train_exog):\n",
    "        b_index, b_value, b_groups, var = 999, 999, None, 'y'\n",
    "        b_score = self.LeafScore(train)\n",
    "        avg = np.mean(train, axis=0)[:-1]\n",
    "        sigma = np.std(train, axis=0)[:-1]\n",
    "        split_data = train, train_exog\n",
    "        for index in range(len(avg)):\n",
    "            for i in range(len(self._erf)):\n",
    "\n",
    "                value = avg[index] + sigma[index] * self._erf[i]\n",
    "                groups = self.test_split(index, value, train)\n",
    "                data = self.rest_split(index, value, train, train_exog,\"n\", 0)\n",
    "                \n",
    "                new_score = 1\n",
    "                for group in groups:\n",
    "                    if len(group) != 0:\n",
    "                        new_score *= self.LeafScore(group)\n",
    "            \n",
    "                        if new_score > b_score:\n",
    "                            b_index, b_value, b_score, b_groups, var, split_data = index, value, new_score, groups, (\"y\"+str(index)), data\n",
    "        j = 0\n",
    "        bb_score = 1\n",
    "        for ex in train_exog:\n",
    "            # print(j)\n",
    "            avg = np.mean(ex, axis=0)[:-1]\n",
    "            bb_score =  max(bb_score, self.LeafScore(ex))\n",
    "            # print(avg)\n",
    "            sigma = np.std(ex, axis=0)[:-1]\n",
    "            for index in range(len(avg)):\n",
    "                for i in range(len(self._erf)):\n",
    "                    value = avg[index] + sigma[index] * self._erf[i]\n",
    "                    groups = self.test_split(index, value, ex)\n",
    "                    data = self.rest_split(index, value, train, train_exog,\"y\", j)\n",
    "                    new_score = 1\n",
    "                    for group in groups:\n",
    "                        if len(group) != 0:\n",
    "                            new_score *= self.LeafScore(group)\n",
    "                \n",
    "                            if new_score > b_score:\n",
    "                                b_index, b_value, b_score, b_groups, var, split_data = index, value, new_score, groups, (\"x\"+str(j)+str(index)), data\n",
    "            j += 1\n",
    "        # print({'index':b_index, 'variable': var, 'value':b_value, 'groups':b_groups})\n",
    "        # print(var, b_score, bb_score)\n",
    "        # print(np.asarray(data).shape)\n",
    "        # self.exog = exog\n",
    "        return {'index':b_index, 'variable': var, 'value':b_value, 'groups':b_groups, 'split_data':split_data}\n",
    "    # turns a group of points, belonging to one datagroup into a terminal node, calculates the parameters for that specific group\n",
    "    def to_terminal(self, target, exog):\n",
    "        outcomes = self.param2(target,exog)\n",
    "        return outcomes\n",
    "    # this recursivelky builds the tree up. If the node should be a terminal node make terminal, else use get split to find next best split\n",
    "    def split(self, node, max_depth, min_size, depth):\n",
    "        left, right = node['groups']\n",
    "        left_t, right_t, left_e, right_e = node['split_data']\n",
    "        del(node['groups'])\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left_t + right_t, left_e + right_e)\n",
    "            return\n",
    "        \n",
    "        if depth >= max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left_t,left_e ), self.to_terminal(right_t, right_e)\n",
    "            return\n",
    "        \n",
    "        if len(left) <= min_size:\n",
    "            node['left'] = self.to_terminal(left_t, left_e)\n",
    "        else:\n",
    "            node['left'] = self.get_split(left_t, left_e)\n",
    "            if node['left']['groups'] is None:\n",
    "                node['left'] = self.to_terminal(left_t, left_e)\n",
    "            else:\n",
    "                self.split(node['left'], max_depth, min_size, depth+1)\n",
    "        \n",
    "        if len(right) <= min_size:\n",
    "            node['right'] = self.to_terminal(right_t, right_e)\n",
    "        else:\n",
    "            node['right'] = self.get_split(right_t, right_e)\n",
    "            if node['right']['groups'] is None:\n",
    "                node['right'] = self.to_terminal(right_t, right_e)\n",
    "            else:\n",
    "                self.split(node['right'], max_depth, min_size, depth+1)\n",
    "    # initiates the buiilding process. Finds initial split and if there are no effective splits then makes source node a terminal node\n",
    "    def build_tree(self, train, train_exog, max_depth, min_size):\n",
    "        train=train\n",
    "        train_exog = train_exog\n",
    "        root = self.get_split(train, train_exog)\n",
    "        if root['groups'] is None:\n",
    "            root['root'] = self.to_terminal(train,train_exog)\n",
    "            root['index'] = None\n",
    "            root['value'] = None\n",
    "            del(root['groups'])\n",
    "        else:\n",
    "            # print(root['index'])\n",
    "            self.split(root, max_depth, min_size, 1)\n",
    "        \n",
    "        return root\n",
    "    # prints the tree structure \n",
    "    def print_tree(self, node, depth=0):\n",
    "        if isinstance(node, dict):\n",
    "            if node['value'] is None:\n",
    "                print(node)\n",
    "                return                                                                                                                               \n",
    "            print('%s[%s < %.3f]' % ((depth*' ', (node['variable']), node['value'])))\n",
    "            # print(depth)\n",
    "            # print(node)\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "    \n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*' ', node)))\n",
    "    # follows the tree strarting from root node until a terminal node is found\n",
    "    def predict(self, node, row):\n",
    "        # Expecting the input row to include lags of both y and X\n",
    "        if 'root' in node:\n",
    "            return node['root']\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "    def param2(self, target, exog):\n",
    "        self.target, self.exog = target, exog\n",
    "    \n",
    "    # Constructing data similar to your earlier approach\n",
    "        data = []\n",
    "        for t, e in zip(target, np.asarray(exog).transpose(1, 0, 2)):\n",
    "            data.append(np.concatenate([t, e.flatten()]))\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Estimation for ARX parameters\n",
    "        N = data.shape[0]\n",
    "        uN_ = self.sample_mean(data)\n",
    "        SN = self.scatter_matrix(data, uN_)\n",
    "        W0 = np.identity(SN.shape[0])\n",
    "        WN = self.WN_func(uN_, SN, W0, N)\n",
    "        ut, Wt_inv = self.MAP_param(N, uN_, WN)\n",
    "        W = inv(Wt_inv)\n",
    "        \n",
    "        var = 1 / W[-1, -1]\n",
    "        Wpp = inv(Wt_inv[:-1, :-1])\n",
    "        b = np.zeros([self._p, 1])\n",
    "        for j in range(SN.shape[0]):\n",
    "            for i in range(self._p):\n",
    "                b[j] += Wt_inv[-1, i] * Wpp[i, j]\n",
    "        \n",
    "        m = ut[-1]\n",
    "        for i in range(SN.shape[0]):\n",
    "            m += b[i] * ut[i]\n",
    "        \n",
    "        return var, b, m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv, det\n",
    "from scipy.special import gamma, erfinv\n",
    "from math import pi, ceil\n",
    "\n",
    "class AutoregressiveTree:\n",
    "    \n",
    "    def __init__(self, p, u0=0, alpha_u=1, X=None):\n",
    "        self._X = X  # Exogenous data\n",
    "        \n",
    "        erf_temp = np.zeros([7,1])\n",
    "        for i in range(1,8):\n",
    "            erf_temp[i-1] = erfinv((i/4) - 1)\n",
    "        \n",
    "        self._erf = erf_temp\n",
    "        self._p = p\n",
    "        self._alpha_W = p + 2\n",
    "        self._u0 = u0\n",
    "        self._alpha_u = alpha_u\n",
    "        self.target = []\n",
    "        self.exog = []\n",
    "\n",
    "    def sample_mean(self, data):\n",
    "        return sum(np.asarray(data))/len(data)\n",
    "\n",
    "    def scatter_matrix(self, data, uN_):\n",
    "        temp = data - uN_\n",
    "        SN = 0\n",
    "        for row in temp:\n",
    "            row = row[:, np.newaxis]\n",
    "            SN += row * row.T\n",
    "        return SN\n",
    "\n",
    "    def WN_func(self, uN_, SN, W0, N):\n",
    "        temp = self._u0 - uN_\n",
    "        temp = temp[:, np.newaxis]\n",
    "        WN = W0 + SN + ((self._alpha_u * N) / (self._alpha_u + N)) * np.dot(temp, temp.T)\n",
    "        return WN\n",
    "\n",
    "    def WN_d_func(self, u0_, uN_d_, SN_d_, W0_, N_):\n",
    "        temp = u0_ - uN_d_\n",
    "        temp = temp[:, np.newaxis]\n",
    "        WN_ = W0_ + SN_d_ + ((self._alpha_u * N_) / (self._alpha_u + N_)) * np.dot(temp, temp.T)\n",
    "        return WN_\n",
    "\n",
    "    def MAP_param(self, N, uN_, WN):\n",
    "        ut = ((self._alpha_u * self._u0) + (N * uN_)) / (self._alpha_u + N)\n",
    "        Wt_inv = (1 / (self._alpha_W + N - (self._p + 1))) * WN\n",
    "        return ut, Wt_inv\n",
    "\n",
    "    def param(self, target, exog):\n",
    "        self.target, self.exog = target, exog\n",
    "        data = []\n",
    "        for t, e in zip(target, np.asarray(exog).transpose(1, 0, 2)):\n",
    "            data.append(np.concatenate([t, e.flatten()]))\n",
    "        data = np.asarray(data)\n",
    "        N = len(data)\n",
    "        uN_ = self.sample_mean(data)\n",
    "        SN = self.scatter_matrix(data, uN_)\n",
    "        W0 = np.identity(SN.shape[0])\n",
    "        WN = self.WN_func(uN_, SN, W0, N)\n",
    "        ut, Wt_inv = self.MAP_param(N, uN_, WN)\n",
    "        W = inv(Wt_inv)\n",
    "        var = 1 / W[-1, -1]\n",
    "        Wpp = inv(Wt_inv[:-1, :-1])\n",
    "        b = np.zeros([self._p, 1])\n",
    "        for j in range(len(b)):\n",
    "            for i in range(self._p):\n",
    "                b[j] += Wt_inv[-1, i] * Wpp[i, j]\n",
    "        m = ut[-1]\n",
    "        for i in range(self._p):\n",
    "            m += b[i] * ut[i]\n",
    "        return var, b, m[0]\n",
    "\n",
    "    def c_func(self, l, alpha):\n",
    "        c = 1\n",
    "        for i in range(1, l + 1):\n",
    "            c *= gamma((alpha + 1 - i) / 2)\n",
    "        return c\n",
    "\n",
    "    def pds_func(self, N, W0, WN):\n",
    "        pds = (pi**(-((self._p + 1) * N) / 2)) + \\\n",
    "        ((self._alpha_u / (self._alpha_u + N))**((self._p + 1) / 2)) + \\\n",
    "        (self.c_func(self._p + 1, self._alpha_W + N) / self.c_func(self._p + 1, self._alpha_W)) * (det(W0)**(self._alpha_W / 2))*(det(WN)**(-(self._alpha_W + N) / 2))\n",
    "        return pds\n",
    "\n",
    "    def pd_s_func(self, N_, W0_, WN_):\n",
    "        pds = (pi**(-((self._p + 1) * N_) / 2)) + \\\n",
    "        ((self._alpha_u / (self._alpha_u + N_))**((self._p + 1) / 2)) + \\\n",
    "        (self.c_func(self._p + 1, self._alpha_W - 1 + N_) / self.c_func(self._p + 1, self._alpha_W - 1)) * (det(W0_)**((self._alpha_W - 1) / 2))*(det(WN_)**(-(self._alpha_W - 1 + N_) / 2))\n",
    "        return pds\n",
    "\n",
    "    def mult_func(self, l, alpha, N):\n",
    "        c = 1\n",
    "        for i in range(1, l + 1):\n",
    "            c *= ((alpha + 1 + N - i)/(alpha + 1 - i))\n",
    "        return c\n",
    "\n",
    "    def pds_func2(self, N, W0, WN, N_, W0_, WN_):\n",
    "        pds = (det(W0)**(self._alpha_W / 2))*det(WN)**(-(self._alpha_W + N) / 2) / (det(W0_)**((self._alpha_W - 1) / 2))*(det(WN_)**(-(self._alpha_W - 1 + N_) / 2)) * \\\n",
    "        self.mult_func(self._p + 1,self._alpha_W, N)\n",
    "        return pds\n",
    "\n",
    "    def pds_func2_singular(self, N, W0, WN, N_, W0_, WN_):\n",
    "        if np.isscalar(WN) or WN.ndim == 0:\n",
    "            det_WN = WN\n",
    "        else:\n",
    "            det_WN = det(WN)\n",
    "        if np.isscalar(WN_) or WN_.ndim == 0:\n",
    "            det_WN_ = WN_\n",
    "        else:\n",
    "            det_WN_ = det(WN_)\n",
    "        pds = ((W0)**(self._alpha_W / 2))*det_WN**(-(self._alpha_W + N) / 2) / ((W0_)**((self._alpha_W - 1) / 2))*(det_WN_**(-(self._alpha_W - 1 + N_) / 2)) * \\\n",
    "        self.mult_func(self._p + 1,self._alpha_W, N)\n",
    "        return pds\n",
    "\n",
    "    def LeafScore(self, data):\n",
    "        self.exog = data\n",
    "        N = len(data)\n",
    "        exog = False\n",
    "        data_ = []\n",
    "        try:\n",
    "            check = (len(data[0]) > 2)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"EXOG\")\n",
    "            exog = True       \n",
    "\n",
    "        if exog:\n",
    "            data_ = (data[:-1])\n",
    "            N_ = len(data_)\n",
    "            uN_ = self.sample_mean(data)\n",
    "            uN_d_ = self.sample_mean(data_)\n",
    "            SN = np.std(data)\n",
    "            W0 = 1\n",
    "            temp = np.asarray(-uN_)\n",
    "            WN = 1 + 1 + ((self._alpha_u * N) / (self._alpha_u + N)) * np.dot(temp, temp.T)\n",
    "            SN = np.std(data_)\n",
    "            W0_ = 1\n",
    "            temp = np.asarray(-uN_d_)\n",
    "            WN_d_ = 1 + 1 + ((self._alpha_u * N_) / (self._alpha_u + N_)) * np.dot(temp, temp.T)\n",
    "        else:\n",
    "            for x in data:\n",
    "                data_.append(x[:-1])\n",
    "            N_ = len(data_)\n",
    "            uN_ = self.sample_mean(data)\n",
    "            uN_d_ = self.sample_mean(data_)\n",
    "            SN = self.scatter_matrix(data, uN_)\n",
    "            W0 = np.identity(SN.shape[0])\n",
    "            WN = self.WN_func(uN_, SN, W0, N)\n",
    "            SN_d_ = self.scatter_matrix(data_, uN_d_)\n",
    "            W0_ = inv(inv(W0)[:-1, :-1])\n",
    "            ut, Wt_inv = self.MAP_param(N, uN_, WN)\n",
    "            u0_ = ut[:-1]\n",
    "            WN_d_ = self.WN_d_func(u0_, uN_d_, SN_d_, W0_, N_)\n",
    "\n",
    "        if N > 20:\n",
    "            if exog:\n",
    "                pds2 = self.pds_func2_singular(N, W0, WN, N_, W0_, WN_d_)\n",
    "            else:\n",
    "                pds2 = self.pds_func2(N, W0, WN, N_, W0_, WN_d_)\n",
    "        else:\n",
    "            pds = self.pds_func(N, W0, WN)\n",
    "            pds_ = self.pd_s_func(N_, W0_, WN_d_)\n",
    "            pds2 =  pds/pds_\n",
    "\n",
    "        return pds2\n",
    "\n",
    "    def test_split(self, index, value, dataset):\n",
    "        exog = False\n",
    "        try:\n",
    "            check = isinstance(len(dataset[0]), list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"EXOG\")\n",
    "            exog = True    \n",
    "        if exog == False:\n",
    "            left, right = list(), list()\n",
    "            for row in dataset:\n",
    "                if row[index] < value:\n",
    "                    left.append(row)\n",
    "                else:\n",
    "                    right.append(row)\n",
    "        else:\n",
    "            left, right = list(), list()\n",
    "            for row in dataset:\n",
    "                if row < value:\n",
    "                    left.append(row)\n",
    "                else:\n",
    "                    right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    def rest_split(self, index, value, train, exog, ex, var):\n",
    "        left, right = [], []\n",
    "        left_e, right_e = [], []\n",
    "\n",
    "        train_df = pd.DataFrame(train)\n",
    "        exog_dfs = [pd.DataFrame(ex) for ex in exog]\n",
    "\n",
    "        if ex == \"y\":\n",
    "            for row in exog_dfs[var]:\n",
    "                if row < value:\n",
    "                    left_e.append(row)\n",
    "                else:\n",
    "                    right_e.append(row)\n",
    "            left_e_index = pd.Index(left_e)\n",
    "            right_e_index = pd.Index(right_e)\n",
    "\n",
    "            left_exog = [df.loc[left_e_index] for df in exog_dfs]\n",
    "            right_exog = []\n",
    "            for df in exog_dfs:\n",
    "                try:\n",
    "                    right_exog.append(df.loc[right_e_index])\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error with DataFrame: \\n{df}\\nError: {e}\")\n",
    "            left = train_df.loc[left_e_index]\n",
    "            right = train_df.loc[right_e_index]\n",
    "        else:\n",
    "            for i, row in train_df.iterrows():\n",
    "                if row[index] < value:\n",
    "                    left.append(i)\n",
    "                else:\n",
    "                    right.append(i)\n",
    "            \n",
    "            left_index = pd.Index(left)\n",
    "            right_index = pd.Index(right)\n",
    "\n",
    "            left = train_df.loc[left_index]\n",
    "            right = train_df.loc[right_index]\n",
    "\n",
    "            left_exog = [df.loc[left_index] for df in exog_dfs]\n",
    "            right_exog = [df.loc[right_index] for df in exog_dfs]\n",
    "\n",
    "        left = left.values.tolist()\n",
    "        right = right.values.tolist()\n",
    "        left_exog = [df.values.tolist() for df in left_exog]\n",
    "        right_exog = [df.values.tolist() for df in right_exog]\n",
    "        return left, right, left_exog, right_exog\n",
    "\n",
    "    def get_split(self, train, train_exog):\n",
    "        b_index, b_value, b_groups, var = 999, 999, None, 'y'\n",
    "        b_score = self.LeafScore(train)\n",
    "        avg = np.mean(train, axis=0)[:-1]\n",
    "        sigma = np.std(train, axis=0)[:-1]\n",
    "        split_data = train, train_exog\n",
    "        for index in range(len(avg)):\n",
    "            for i in range(len(self._erf)):\n",
    "                value = avg[index] + sigma[index] * self._erf[i]\n",
    "                groups = self.test_split(index, value, train)\n",
    "                data = self.rest_split(index, value, train, train_exog, 'x', var)\n",
    "                gini = self.LeafScore(data[0])\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups, var = index, value, gini, data, 'x'\n",
    "        if not isinstance(train_exog[0], list):\n",
    "            train_exog = [train_exog]\n",
    "        exog_avg = [np.mean(exog, axis=0) for exog in train_exog]\n",
    "        exog_sigma = [np.std(exog, axis=0) for exog in train_exog]\n",
    "        for j, ex in enumerate(train_exog):\n",
    "            for index in range(len(exog_avg[j])):\n",
    "                for i in range(len(self._erf)):\n",
    "                    value = exog_avg[j][index] + exog_sigma[j][index] * self._erf[i]\n",
    "                    groups = self.test_split(index, value, ex)\n",
    "                    data = self.rest_split(index, value, train, train_exog, 'y', j)\n",
    "                    gini = self.LeafScore(data[0])\n",
    "                    if gini < b_score:\n",
    "                        b_index, b_value, b_score, b_groups, var = index, value, gini, data, j\n",
    "        return {'index':b_index, 'value':b_value, 'groups':b_groups, 'var':var}\n",
    "    def to_terminal(self, target, exog):\n",
    "        outcomes = self.param(target, exog)\n",
    "        return outcomes\n",
    "\n",
    "    def split(self, node, max_depth, min_size, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right, [])\n",
    "            return\n",
    "\n",
    "        if depth >= max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left, []), self.to_terminal(right, [])\n",
    "            return\n",
    "\n",
    "        if len(left) <= min_size:\n",
    "            node['left'] = self.to_terminal(left, [])\n",
    "        else:\n",
    "            node['left'] = self.get_split(left, [])\n",
    "            self.split(node['left'], max_depth, min_size, depth+1)\n",
    "\n",
    "        if len(right) <= min_size:\n",
    "            node['right'] = self.to_terminal(right, [])\n",
    "        else:\n",
    "            node['right'] = self.get_split(right, [])\n",
    "            self.split(node['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "    def build_tree(self, train, train_exog, max_depth, min_size):\n",
    "        root = self.get_split(train, train_exog)\n",
    "        self.split(root, max_depth, min_size, 1)\n",
    "        return root\n",
    "\n",
    "    def print_tree(self, node, depth=0):\n",
    "        if isinstance(node, dict):\n",
    "            if node['value'] is None:\n",
    "                print(node)\n",
    "                return                                                                                                                               \n",
    "            print('%s[%s < %.3f]' % ((depth*' ', (node['variable']), node['value'])))\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "    def predict(self, node, row):\n",
    "        if 'root' in node:\n",
    "            return node['root']\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 32, 36, 37] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\Code\\ARX.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# d_exog = d_exog.T\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m ART \u001b[39m=\u001b[39m AutoregressiveTree(\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m tree \u001b[39m=\u001b[39m ART\u001b[39m.\u001b[39;49mbuild_tree(d, synthetic_exog\u001b[39m.\u001b[39;49mto_numpy(), \u001b[39m10\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m ART\u001b[39m.\u001b[39mprint_tree(tree)\n",
      "\u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\Code\\ARX.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=319'>320</a>\u001b[0m train\u001b[39m=\u001b[39mtrain\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=320'>321</a>\u001b[0m train_exog \u001b[39m=\u001b[39m train_exog\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=321'>322</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_split(train, train_exog)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=322'>323</a>\u001b[0m \u001b[39mif\u001b[39;00m root[\u001b[39m'\u001b[39m\u001b[39mgroups\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=323'>324</a>\u001b[0m     root[\u001b[39m'\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_terminal(train,train_exog)\n",
      "\u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\Code\\ARX.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=246'>247</a>\u001b[0m value \u001b[39m=\u001b[39m avg[index] \u001b[39m+\u001b[39m sigma[index] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_erf[i]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m groups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_split(index, value, train)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_split(index, value, train, train_exog,\u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m new_score \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m groups:\n",
      "\u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\Code\\ARX.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m     left \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mloc[left_index]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m     right \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mloc[right_index]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m     left_exog \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39;49mloc[left_index] \u001b[39mfor\u001b[39;49;00m df \u001b[39min\u001b[39;49;00m exog_dfs]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m     right_exog \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mloc[right_index] \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m exog_dfs]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m left \u001b[39m=\u001b[39m left\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\Code\\ARX.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m     left \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mloc[left_index]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=227'>228</a>\u001b[0m     right \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mloc[right_index]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m     left_exog \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39;49mloc[left_index] \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m exog_dfs]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m     right_exog \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mloc[right_index] \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m exog_dfs]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/tiesh/Documents/Thesis_CD/DyART/Code/ARX.ipynb#W4sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m left \u001b[39m=\u001b[39m left\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1330\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1334\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1271\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1273\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1459\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1460\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1462\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1464\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiesh\\Documents\\Thesis_CD\\DyART\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: '[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 29, 30, 32, 36, 37] not in index'"
     ]
    }
   ],
   "source": [
    "p = 3\n",
    "df_temp2 = pd.concat([synthetic_target,synthetic_exog], axis=1)\n",
    "df_temp2.columns = range(df_temp2.shape[1])\n",
    "\n",
    "ts_train, ts_valid, ts_test, ts_param = preprocessing(df_temp2, method='normalization')\n",
    "\n",
    "train = []\n",
    "for s in ts_train:\n",
    "    temp = []\n",
    "    for i in range(len(s) - (p + 1)):\n",
    "        temp.append(s[i:i + p + 1])\n",
    "\n",
    "\n",
    "    train.append(temp)\n",
    "d = train[0]\n",
    "# print(len(d), len(d[0]))\n",
    "d_exog = ts_train[1:]\n",
    "# d_exog = d_exog.T\n",
    "ART = AutoregressiveTree(3)\n",
    "tree = ART.build_tree(d, synthetic_exog.to_numpy(), 10, 5)\n",
    "\n",
    "ART.print_tree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_exog.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 4\n"
     ]
    }
   ],
   "source": [
    "print(len(arr), len(arr[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
